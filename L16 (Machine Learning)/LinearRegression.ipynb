{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a11e15",
   "metadata": {},
   "source": [
    "<!-- File automatically generated using DocOnce (https://github.com/doconce/doconce/):\n",
    "doconce format ipynb LinearRegression.do.txt  -->\n",
    "\n",
    "## Linear regression with neurons\n",
    "We begin by importing some packages that can help us analyse and visualize\n",
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d135d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (4, 2)\n",
    "plt.rcParams['figure.dpi'] = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4c5136",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "The aim of linear regression is to find the straight line that best fits the data $D = (x_1,y)$. \n",
    "This boils down to assuming a linear relationship $y = slope\\, x_1 + intercept$ and finding the\n",
    "$slope$ and $intercept$ which makes the estimated $\\hat{y}$ as close as possible to $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b5d561",
   "metadata": {},
   "source": [
    "## Training a linear rate neuron\n",
    "\n",
    "<!-- dom:FIGURE: [figures/linear-rate-neuron.png] <div id=\"fig:linear-rate-neuron\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:linear-rate-neuron\"></div>\n",
    "\n",
    "<img src=\"figures/linear-rate-neuron.png\" ><p style=\"font-size: 0.9em\"><i>Figure 1</i></p>\n",
    "<!-- end figure -->\n",
    "\n",
    "If we define $\\sigma(x)=x$ we see that the output corresponds to a straight line $\\hat{y}=\\sigma(z)=w_0 + w_1x_1$.\n",
    "Think about these as numbers i.e. $w_0,w_1,x_1\\in\\mathbb{R}$ that flow through the neuron one by one.\n",
    "We now have to fit the weights $w_0, w_1$ to the $intercept$ and $slope$ respectably. \n",
    "To do this we first need to define an objective function. \n",
    "Our objective is to make $\\hat{y}$ and $y$ as close as possible which can be measured with the\n",
    "square error, or \"loss\" $\\mathcal{L}=\\frac{1}{2}(y - \\hat{y})^2$. \n",
    "When the loss is at its minimum the derivatives $\\frac{\\partial \\mathcal{L}}{\\partial w_0}$ and $\\frac{\\partial \\mathcal{L}}{\\partial w_1}$ are zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3cd58c",
   "metadata": {},
   "source": [
    "## Exercise 1: Compute the derivatives of the loss\n",
    "\n",
    "We can now use the chain rule\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z} \\frac{\\partial z}{\\partial w}$ on the functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96a7bee",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L} &= \\frac{1}{2}(y - \\hat{y})^2\\\\\n",
    "\\hat{y} &= z\\\\\n",
    "z &= x_1w_1 + w_0\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b1f4b",
   "metadata": {},
   "source": [
    "to derive a rule for minimizing the objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1241671f",
   "metadata": {},
   "source": [
    "Find $\\frac{\\partial \\mathcal{L}}{\\partial w_0}$ and $\\frac{\\partial \\mathcal{L}}{\\partial w_1}$ using the chain rule.\n",
    "\n",
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1581e884",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} &= -(y - \\hat{y}) \\\\\n",
    "\\frac{\\partial \\hat{y}}{\\partial z} &= 1 \\\\\n",
    "\\frac{\\partial z}{\\partial w_0} &= 1 \\\\\n",
    "\\frac{\\partial z}{\\partial w_1} &= x_1 \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_0} &= -(y-\\hat{y})\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_1} &= -x_1 (y-\\hat{y})\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b96a39",
   "metadata": {},
   "source": [
    "<!-- --- end solution of exercise --- -->\n",
    "To make notation a little easier we define $\\mathbf{W} = (w_0, w_1)$ and the gradient as \n",
    "$\\nabla_\\mathbf{W}\\mathcal{L}=(\\frac{\\partial \\mathcal{L}}{\\partial w_0},\\frac{\\partial \\mathcal{L}}{\\partial w_1})$.\n",
    "Now we can do gradient descent by updating the weights by $\\mathbf{W} = \\mathbf{W} - \\gamma \\nabla_\\mathbf{W}\\mathcal{L}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef63134",
   "metadata": {},
   "source": [
    "## Making syntethic training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a324f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of training data\n",
    "N = 200\n",
    "# 200 random samples as our data\n",
    "x_1 = np.random.rand(N)\n",
    "# Define the line slope and the Gaussian noise parameters\n",
    "slope = 3\n",
    "intercept, sigma = 0, 0.1 # mean and standard deviation\n",
    "intercepts = np.random.normal(intercept, sigma, N)\n",
    "# Define the coordinates of the data points using the line equation and the added Gaussian noise \n",
    "y = slope * x_1 + intercepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7124afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we plot our generated training data\n",
    "plt.scatter(x_1, y, c='r', alpha=0.5)\n",
    "plt.grid()\n",
    "plt.title('Data Points')\n",
    "plt.xlabel('x (Our data)')\n",
    "plt.ylabel('y (ground-truth)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61727061",
   "metadata": {},
   "source": [
    "## Exercise 2: Linear regression with stochastic gradient descent\n",
    "\n",
    "Perform gradient descent to estimate $intercept$ and $slope$ with $w_0,w_1$ as defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616b3f38",
   "metadata": {},
   "source": [
    "**a)**\n",
    "Define python functions for the gradients `dL_dw0`, `dl_dw1` and `loss_fn` by filling in the blanks \"___\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaa27cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dL_dw0(y, y_hat):\n",
    "    return ___\n",
    "\n",
    "def dL_dw1(y, y_hat, x_1):\n",
    "    return ___\n",
    "\n",
    "def loss_fn(y, y_hat):\n",
    "    return ___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22f68f",
   "metadata": {},
   "source": [
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc6297",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "def dL_dw0(y, y_hat):\n",
    "    return y_hat - y\n",
    "\n",
    "def dL_dw1(y, y_hat, x_1):\n",
    "    return x_1*(y_hat - y)\n",
    "\n",
    "def loss_fn(y, y_hat):\n",
    "    return 0.5*(y - y_hat)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58166a5",
   "metadata": {},
   "source": [
    "<!-- --- end solution of exercise --- -->\n",
    "Now we have to initialize our weights, lets pick some random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0879e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0, w_1 = -5, 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debd2f9c",
   "metadata": {},
   "source": [
    "**b)**\n",
    "Make a for loop that goes through the inputs in `x_1` and updates `y_hat` according to the loss using gradient descent.\n",
    "Store the loss values for later usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6648576",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.1\n",
    "losses = []\n",
    "for x_1_val, y_val in zip(x_1, y):\n",
    "    y_hat = ___\n",
    "    loss = ___\n",
    "    w_0 = ___\n",
    "    w_1 = ___\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22f68f_1",
   "metadata": {},
   "source": [
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56feb0e1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "gamma = 0.1\n",
    "losses = []\n",
    "for x_1_val, y_val in zip(x_1, y):\n",
    "    y_hat = w_0 + w_1*x_1_val\n",
    "    loss = loss_fn(y_val, y_hat)\n",
    "    w_0 = w_0 - gamma*dL_dw0(y_val, y_hat)\n",
    "    w_1 = w_1 - gamma*dL_dw1(y_val, y_hat, x_1_val)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a5944a",
   "metadata": {},
   "source": [
    "<!-- --- end solution of exercise --- -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef98ebef",
   "metadata": {},
   "source": [
    "**c)**\n",
    "Plot the result superimposed on a scatter plot of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9916d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(5,3.5))\n",
    "# ---> Plot the loss <---\n",
    "\n",
    "\n",
    "# ---> Plot the data points (x_1,y) <---\n",
    "\n",
    "\n",
    "# ---> Plot the model fit in the same plot as the data points <---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22f68f_2",
   "metadata": {},
   "source": [
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e6b363",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(5,3.5))\n",
    "ax1.plot(losses)\n",
    "ax1.set_title('Data Points')\n",
    "ax1.set_xlabel('steps')\n",
    "ax1.set_ylabel('loss')\n",
    "ax2.scatter(x_1, y, c='r', alpha=0.5)\n",
    "ax2.set_title('Model fit')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "x_grid = np.linspace(0,1,2)\n",
    "ax2.plot(x_grid, w_0 + w_1*x_grid)\n",
    "plt.suptitle(f'Slope: true {slope:.3f} est {w_1:.3f},\\nIntercept: true {intercept:.3f} est {w_0:.3f}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c869cef",
   "metadata": {},
   "source": [
    "<!-- --- end solution of exercise --- -->\n",
    "\n",
    "As we can see the descent is stochastic, and does not converge to a stationary value. \n",
    "The reason is that we compute the loss once for every single input value. \n",
    "To mitigate this we can introduce the mean squared error $\\mathcal{L} = \\frac{1}{2n}\\sum_i(y_i - \\hat{y}_i)^2$. \n",
    "Here $n$ is the number of inputs, known as the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589e7539",
   "metadata": {},
   "source": [
    "## Exercise 3: Stochastic gradient descent with batches and epochs\n",
    "\n",
    "Perform gradient descent to estimate $intercept$ and $slope$ with $w_0,w_1$ as defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159295ed",
   "metadata": {},
   "source": [
    "**a)**\n",
    "Define python functions for the gradients `batch_dL_dw0`, `batch_dl_dw1` and `batch_loss_fn`\n",
    "\n",
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd468a0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "def batch_dL_dw0(y, y_hat):\n",
    "    return np.mean(y_hat - y)\n",
    "\n",
    "def batch_dL_dw1(y, y_hat, x_1):\n",
    "    return np.mean(x_1*(y_hat - y))\n",
    "\n",
    "def batch_loss_fn(y, y_hat):\n",
    "    return 0.5*np.mean((y - y_hat)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a5944a_1",
   "metadata": {},
   "source": [
    "<!-- --- end solution of exercise --- -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda70745",
   "metadata": {},
   "source": [
    "**b)**\n",
    "Make a for loop that goes through the inputs in `x_1` and updates `y_hat` according to the loss using gradient descent.\n",
    "Store the loss values for later usage\n",
    "\n",
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2df4f0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "w_0, w_1 = -5, 5\n",
    "gamma = 0.1\n",
    "losses = []\n",
    "n_epochs = 1000\n",
    "n_batch = len(x_1)\n",
    "batch_x_1 = [x_1[n:n + n_batch] for n in range(0, len(x_1), n_batch)]\n",
    "batch_y = [y[n:n + n_batch] for n in range(0, len(y), n_batch)]\n",
    "for epoch in range(n_epochs):\n",
    "    for x_1_val, y_val in zip(batch_x_1, batch_y):\n",
    "        y_hat = w_0 + w_1*x_1_val\n",
    "        losses.append(batch_loss_fn(y_val, y_hat))\n",
    "        w_0 = w_0 - gamma*batch_dL_dw0(y_val, y_hat)\n",
    "        w_1 = w_1 - gamma*batch_dL_dw1(y_val, y_hat, x_1_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a5944a_2",
   "metadata": {},
   "source": [
    "<!-- --- end solution of exercise --- -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadc6d48",
   "metadata": {},
   "source": [
    "**c)**\n",
    "Plot the result superimposed on a scatter plot of the data\n",
    "\n",
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e6b363_1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(5,3.5))\n",
    "ax1.plot(losses)\n",
    "ax1.set_title('Data Points')\n",
    "ax1.set_xlabel('steps')\n",
    "ax1.set_ylabel('loss')\n",
    "ax2.scatter(x_1, y, c='r', alpha=0.5)\n",
    "ax2.set_title('Model fit')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "x_grid = np.linspace(0,1,2)\n",
    "ax2.plot(x_grid, w_0 + w_1*x_grid)\n",
    "plt.suptitle(f'Slope: true {slope:.3f} est {w_1:.3f},\\nIntercept: true {intercept:.3f} est {w_0:.3f}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a5944a_3",
   "metadata": {},
   "source": [
    "<!-- --- end solution of exercise --- -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1812e4",
   "metadata": {},
   "source": [
    "## Visualizing the loss surface\n",
    "In this section we will get a better understanding of the loss surface by visualizing it. This is possible because we are working with a simple linear model with two parameters.\n",
    "\n",
    "You only need to run the following code. Note that the code depends on the random initialization and will give, therefore, different results every time you run it, so we recommend that you run it a few times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dc7637",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 100\n",
    "w0_grid = np.linspace(-10,10,resolution)\n",
    "w1_grid = np.linspace(-10,10,resolution)\n",
    "w0_w1_2dgrid = np.stack(np.meshgrid(w0_grid, w1_grid), axis=-1) # shape (resolution,resolution,2)\n",
    "w0_w1_2dgrid = w0_w1_2dgrid.reshape(-1,2) # shape (resolution**2,2)\n",
    "\n",
    "y_hat = w0_w1_2dgrid[:,0:1] + w0_w1_2dgrid[:,1:]*x_1[None] # shape (resolution**2,N)\n",
    "loss = np.mean(loss_fn(y[None], y_hat),axis=-1) # shape (resolution**2,)\n",
    "loss_2dgrid = loss.reshape(resolution,resolution) # shape (resolution,resolution)\n",
    "\n",
    "# select a random starting point and compute the (average) gradient at that point to get the direction of steepest descent\n",
    "w0, w1 = np.random.uniform(-10,10,2)\n",
    "y_hat = w0 + w1*x_1\n",
    "dL_dw0_value = np.mean(dL_dw0(y[None], y_hat))\n",
    "dL_dw1_value = np.mean(dL_dw1(y[None], y_hat, x_1))\n",
    "gamma = 0.5\n",
    "\n",
    "# plot the loss surface and the gradient\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(8,4))\n",
    "axs[0].contourf(w0_grid, w1_grid, loss_2dgrid, levels=100)\n",
    "axs[0].set_xlabel('w0')\n",
    "axs[0].set_ylabel('w1')\n",
    "axs[0].set_title('Loss surface')\n",
    "axs[0].scatter(w0, w1, c='r', marker='x')\n",
    "axs[0].arrow(w0, w1, -gamma*dL_dw0_value, -gamma*dL_dw1_value, width=0.1, color='r')\n",
    "\n",
    "# plot the data and the current random model, and its improvement in a series of 10 steps\n",
    "axs[1].scatter(x_1, y, c='r', alpha=0.5)\n",
    "axs[1].set_xlabel('x')\n",
    "axs[1].set_ylabel('y')\n",
    "axs[1].set_title('Model fit')\n",
    "x_grid = np.linspace(0,1,2)\n",
    "# create alpha with a linear gradient to make the lines fade out\n",
    "alpha = np.linspace(0.1,0.5,10)\n",
    "for i in range(10):\n",
    "    y_hat = w0 + w1*x_1\n",
    "    axs[1].plot(x_grid, w0+w1*x_grid, c='b', alpha=alpha[i])\n",
    "    dL_dw0_value = np.mean(dL_dw0(y[None], y_hat))\n",
    "    dL_dw1_value = np.mean(dL_dw1(y[None], y_hat, x_1[None]))\n",
    "    w0_next = w0 - gamma*dL_dw0_value\n",
    "    w1_next = w1 - gamma*dL_dw1_value\n",
    "    # plot more gradients\n",
    "    axs[0].arrow(w0, w1, w0_next-w0, w1_next-w1, width=0.1, color='r')\n",
    "    w0, w1 = w0_next, w1_next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe6b4d0",
   "metadata": {},
   "source": [
    "## Gradient descent with mean squared error equivalent to maximum likelihood minimization\n",
    "<!-- https://d2l.ai/chapter_linear-regression/linear-regression.html#the-normal-distribution-and-squared-loss -->\n",
    "Minimizing the mean squared error (MSE) is equivalent to maximum likelihood estimation (MLE) of a linear model under the assumption of additive Gaussian noise which we will show here.\n",
    "\n",
    "To begin with, we have a simple linear model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb34331",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y_i = \\boldsymbol{w}^T \\boldsymbol{x}_i + \\epsilon_i,\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d653be",
   "metadata": {},
   "source": [
    "Here, $y_i$ represents the observed output for the $i$-th instance. $\\boldsymbol{w}$ is a weight vector, $\\boldsymbol{x}_i$ is the $i$-th input vector, and $\\epsilon_i$ denotes the additive Gaussian noise, which is assumed to have zero mean and a variance of $\\sigma^2$. \n",
    "\n",
    "The probability density function of the Gaussian noise is expressed as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c1b007",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "p(\\epsilon_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{\\epsilon_i^2}{2\\sigma^2}\\right).\n",
    "\\tag{2}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ec1d06",
   "metadata": {},
   "source": [
    "This Gaussian function models the probability distribution of the noise term in the model. \n",
    "\n",
    "The important consequence of our linear model and Gaussian noise assumption is that the conditional distribution of $y_i$ given $\\boldsymbol{x}_i$ and $\\boldsymbol{w}$ also follows a Gaussian distribution. We express it as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da71abb3",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "p(y_i|\\boldsymbol{x}_i, \\boldsymbol{w}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(y_i - \\boldsymbol{w}^T \\boldsymbol{x}_i)^2}{2\\sigma^2}\\right).\n",
    "\\tag{3}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa75c6aa",
   "metadata": {},
   "source": [
    "When performing maximum likelihood estimation (MLE), we aim to find the parameters (here, $\\boldsymbol{w}$) that maximize the likelihood function, which is the product of the probability densities for each data point in our dataset. The likelihood function is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f21ab8d",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto4\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "L(\\boldsymbol{w}) = \\prod_{i=1}^N p(y_i|\\boldsymbol{x}_i, \\boldsymbol{w}) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(y_i - \\boldsymbol{w}^T \\boldsymbol{x}_i)^2}{2\\sigma^2}\\right).\n",
    "\\tag{4}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c91f83a",
   "metadata": {},
   "source": [
    "Multiplying many small numbers can lead to numerical instability, and taking the derivative of a product can be cumbersome. Taking the logarithm of the likelihood function is a common way to mitigate both of these issues, \n",
    "converting products into sums and turning products of derivatives into sums of derivatives. Moreover, since the logarithm is a monotonically increasing function, maximizing the likelihood is equivalent to maximizing the log-likelihood.\n",
    "The log-likelihood is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffcad8f",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto5\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\ln L(\\boldsymbol{w}) = \\sum_{i=1}^N \\ln p(y_i|\\boldsymbol{x}_i, \\boldsymbol{w}) = -\\frac{N}{2} \\ln (2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^N (y_i - \\boldsymbol{w}^T \\boldsymbol{x}_i)^2.\n",
    "\\tag{5}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f1b39",
   "metadata": {},
   "source": [
    "Notice that maximizing the log-likelihood is equivalent to minimizing the negative of it. Also, since the first term does not depend on $\\boldsymbol{w}$, we can ignore it during the optimization process. \n",
    "Moreover, since $\\sigma^2$ is a constant, it does not affect the optimization either and we can ignore it as well.\n",
    "This leads us to an optimization problem that resembles the mean squared error:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b89a734",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto6\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\hat{\\boldsymbol{w}} = \\arg\\min_{\\boldsymbol{w}} \\sum_{i=1}^N (y_i - \\boldsymbol{w}^T \\boldsymbol{x}_i)^2.\n",
    "\\tag{6}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dfafa5",
   "metadata": {},
   "source": [
    "Finally, for the sake of completeness, we divide by $N$ to get the 'mean' squared error (MSE), showing that minimizing the MSE is equivalent to performing MLE under our assumptions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97144a14",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto7\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\hat{\\boldsymbol{w}} = \\arg\\min_{\\boldsymbol{w}} \\frac{1}{N}\\sum_{i=1}^N (y_i - \\boldsymbol{w}^T \\boldsymbol{x}_i)^2.\n",
    "\\tag{7}\n",
    "\\end{equation}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
