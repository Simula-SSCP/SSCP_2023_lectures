{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e4c84a6",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- File automatically generated using DocOnce (https://github.com/doconce/doconce/):\n",
    "doconce format ipynb MLP.do.txt  -->\n",
    "\n",
    "## Exploring Multi-Layer Perceptrons (MLP) and the XOR Problem\n",
    "This notebook takes inspiration from the following sources:\n",
    "1. <https://towardsdatascience.com/how-neural-networks-solve-the-xor-problem-59763136bdd7> (2023)\n",
    "2. <https://www.quora.com/Why-cant-the-XOR-problem-be-solved-by-a-one-layer-perceptron> (2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d135d69",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (4, 2)\n",
    "plt.rcParams['figure.dpi'] = 150"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c4bc58f",
   "metadata": {
    "editable": true
   },
   "source": [
    "In their 1969 book, \"Perceptrons: An Introduction to Computational Geometry\" \n",
    "Marvin Minsky and Seymour A. Papert demonstrated that perceptrons could not solve the XOR problem. \n",
    "In this notebook, we will revisit this argument and become more familiar with perceptrons, gradient learning, \n",
    "and the limitations of such simple models. This discussion will also provide a compelling example of why \n",
    "we should consider multi-layer perceptrons instead of single-layer perceptrons. \n",
    "<!-- Finally, we will generalize our example to arbitrary functions using the universal approximation theorem. -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f22e048",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The Perceptron\n",
    "A perceptron is a linear, [single-valued function](https://mathworld.wolfram.com/Single-ValuedFunction.html) \n",
    "$f: \\mathbb{R}^N \\rightarrow \\mathbb{R}$, accompanied by a subsequent thresholding function:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c08cb19f",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "g(x) = \n",
    "\\begin{cases}\n",
    "& 1, \\ x > T \\\\\n",
    "& 0, \\ \\text{otherwise} \\\\\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5950fe6",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $T$ is a threshold value, typically set to 0. We can visualize the thresholding function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7494298f",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def g(x, T=0):\n",
    "    \"\"\"\n",
    "    Thresholding function\n",
    "\n",
    "    Parameters:\n",
    "        x, float: Input to thresholding function\n",
    "        T, float: Threshold value, typically (default) set to 0\n",
    "    Returns:\n",
    "        y, float: Output of thresholding function\n",
    "    \"\"\"\n",
    "    return x > T\n",
    "\n",
    "x = np.linspace(-1,1,100)\n",
    "plt.plot(x, g(x))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('g(x)')\n",
    "plt.title('Thresholding Function')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef5c4bf9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## XOR: The input and target\n",
    "The XOR function maps two binary values to a third binary value, \n",
    "$f: \\{0,1\\}^2 \\rightarrow \\{0,1\\}$, as depicted in the following Boolean logic table:\n",
    "\n",
    "<table class=\"dotable\" border=\"1\">\n",
    "<thead>\n",
    "<tr><th align=\"center\">A</th> <th align=\"center\">B</th> <th align=\"center\">XOR</th> </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td align=\"center\">   0    </td> <td align=\"center\">   0    </td> <td align=\"center\">   0      </td> </tr>\n",
    "<tr><td align=\"center\">   0    </td> <td align=\"center\">   1    </td> <td align=\"center\">   1      </td> </tr>\n",
    "<tr><td align=\"center\">   1    </td> <td align=\"center\">   0    </td> <td align=\"center\">   1      </td> </tr>\n",
    "<tr><td align=\"center\">   1    </td> <td align=\"center\">   1    </td> <td align=\"center\">   0      </td> </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "We begin by defining the domain-codomain (input-outputs) of the XOR function. This provides the inputs and \n",
    "targets that our function should try to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f316a1ee",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def xor():\n",
    "    \"\"\"\n",
    "    Define the input and output of the XOR-function\n",
    "\n",
    "    Returns:\n",
    "        A_B, XOR: A 2d-tuple of the input-output pairs of the XOR-function\n",
    "    \"\"\"\n",
    "    A_B = np.array([\n",
    "        [0,0],\n",
    "        [0,1],\n",
    "        [1,0],\n",
    "        [1,1]\n",
    "    ])\n",
    "    XOR = np.array([\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0\n",
    "    ])\n",
    "    return A_B, XOR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31059efa",
   "metadata": {
    "editable": true
   },
   "source": [
    "Let us plot the input-output pairs of the XOR function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53867dee",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# get input-output pairs\n",
    "A_B, XOR = xor()\n",
    "# plot the input-output pairs\n",
    "plt.scatter(A_B[:,0], A_B[:,1], c=XOR, cmap='bwr')\n",
    "plt.xlabel('A')\n",
    "plt.ylabel('B')\n",
    "plt.title('XOR')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb6f3980",
   "metadata": {
    "editable": true
   },
   "source": [
    "Before moving on, can you think of a way to draw a line that separates the input-output pairs of the XOR function?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "574adffa",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 1: Creating a 2D Perceptron\n",
    "\n",
    "To build the perceptron we will use the Variable class that we created in the previous notebook. We import this class from the variable module."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7684988",
   "metadata": {
    "editable": true
   },
   "source": [
    "Following, we provide you with a skeleton for a perceptron. Your task is to fill in the missing \"___\" parts in the \n",
    "`forward`, `evaluate` and `loss_fn` functions.\n",
    "\n",
    "*Note that: We set the bias to -1 to arrive at a an interesting solution where the \n",
    "decision boundary lives near the XOR data points.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8ac430",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from variable import Variable\n",
    "\n",
    "class Perceptron2D:\n",
    "    def __init__(self, T=0, lr=1e-1):\n",
    "        \"\"\"\n",
    "        Two dimensional Percetron (we assume only two weights and thus two inputs) for \n",
    "        attempting to learn the XOR-function.\n",
    "\n",
    "        Parameters:\n",
    "            T, float: Threshold value, typically (default) set to 0\n",
    "            lr, float: Learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        self.T, self.lr = T, lr\n",
    "        # initialise weights and biases - here from a random uniform distribution with range [0,1] \n",
    "        self.w1 = Variable(np.random.uniform())\n",
    "        self.w2 = Variable(np.random.uniform())\n",
    "        self.bias = Variable(-1)\n",
    "    \n",
    "    def forward(self,A,B):\n",
    "        \"\"\"\n",
    "        Evaluates the (linear part of the) perceptron for a given input\n",
    "\n",
    "        Parameters:\n",
    "            A,B, Variable: Current input to perceptron\n",
    "        Returns:\n",
    "            y_hat, Variable: A Variable object following the perceptron auto-diff graph\n",
    "        \"\"\"\n",
    "        y_hat = ___\n",
    "        return y_hat\n",
    "\n",
    "    def evaluate(self,A,B):\n",
    "        \"\"\"\n",
    "        Extends the forward function to include thresholding (full Perceptron forward)\n",
    "\n",
    "        Parameters:\n",
    "            A,B, Variable: Current input to perceptron\n",
    "        Returns:\n",
    "            y_pred, bool: Boolean (True/False or equivalently 0/1), the (full) forward pass of the perceptron\n",
    "        \"\"\"\n",
    "        y_hat = ___\n",
    "        # NOTE! \">\" is not defined for Variable objects, so we need to extract the value\n",
    "        return ___\n",
    "\n",
    "    def loss_fn(self,A,B,y):\n",
    "        \"\"\"\n",
    "        Computes the loss function for a given input\n",
    "\n",
    "        Parameters:\n",
    "            A,B, Variable: Current input to perceptron\n",
    "            y, Variable: Target value for the input\n",
    "        Returns:\n",
    "            loss, Variable: A Variable object following the loss auto-diff graph\n",
    "        \"\"\"\n",
    "        y_hat = self.forward(A,B)\n",
    "        loss = (y_hat - y)**2\n",
    "        return loss\n",
    "\n",
    "    def gradient_step(self, A, B, y):\n",
    "        \"\"\"\n",
    "        Performs a gradient step for a given input\n",
    "\n",
    "        Parameters:\n",
    "            A,B, Variable: Current input to perceptron\n",
    "            y, Variable: Target value for the input\n",
    "        \"\"\"\n",
    "        # forward pass\n",
    "        loss = ___\n",
    "        # backward pass\n",
    "        grads = ___\n",
    "        # update weights and omit bias\n",
    "        w1 = self.w1 - self.lr*grads[self.w1]\n",
    "        w2 = self.w2 - self.lr*grads[self.w2]\n",
    "        # we need to reinstantiate the Variable object to avoid infinite recursion\n",
    "        self.w1 = Variable(w1.value)\n",
    "        self.w2 = Variable(w2.value)\n",
    "        return loss.value # to track loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec22f68f",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5dab0b",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "from variable import Variable\n",
    "\n",
    "class Perceptron2D:\n",
    "    def __init__(self, T=0, lr=1e-1):\n",
    "        \"\"\"\n",
    "        Two dimensional Percetron (we assume only two weights and thus two inputs) for \n",
    "        attempting to learn the XOR-function.\n",
    "\n",
    "        Parameters:\n",
    "            T, float: Threshold value, typically (default) set to 0\n",
    "            lr, float: Learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        self.T, self.lr = T, lr\n",
    "        # initialise weights and biases - here from a random uniform distribution with range [0,1] \n",
    "        self.w1 = Variable(np.random.uniform())\n",
    "        self.w2 = Variable(np.random.uniform())\n",
    "        self.bias = Variable(-1)\n",
    "    \n",
    "    def forward(self,A,B):\n",
    "        \"\"\"\n",
    "        Evaluates the (linear part of the) perceptron for a given input\n",
    "\n",
    "        Parameters:\n",
    "            A,B, Variable: Current input to perceptron\n",
    "        Returns:\n",
    "            y_hat, Variable: A Variable object following the perceptron auto-diff graph\n",
    "        \"\"\"\n",
    "        y_hat = A*self.w1 + B*self.w2 + self.bias\n",
    "        return y_hat\n",
    "\n",
    "    def evaluate(self,A,B):\n",
    "        \"\"\"\n",
    "        Extends the forward function to include the thresholding (full Perceptron forward)\n",
    "\n",
    "        Parameters:\n",
    "            A,B, Variable: Current input to perceptron\n",
    "        Returns:\n",
    "            y_pred, bool: Boolean (True/False or equivalently 0/1), the (full) forward pass of the perceptron\n",
    "        \"\"\"\n",
    "        y_hat = self.forward(A,B)\n",
    "        # NOTE! \">\" is not defined for Variable objects, so we need to extract the value\n",
    "        return y_hat.value > self.T\n",
    "\n",
    "    def loss_fn(self,A,B,y):\n",
    "        \"\"\"\n",
    "        Computes the loss function for a given input\n",
    "\n",
    "        Parameters:\n",
    "            A,B, Variable: Current input to perceptron\n",
    "            y, Variable: Target value for the input\n",
    "        Returns:\n",
    "            loss, Variable: A Variable object following the loss auto-diff graph\n",
    "        \"\"\"\n",
    "        y_hat = self.forward(A,B)\n",
    "        loss = (y_hat - y)**2\n",
    "        return loss\n",
    "\n",
    "    def gradient_step(self, A, B, y):\n",
    "        \"\"\"\n",
    "        Performs a gradient step for a given input\n",
    "\n",
    "        Parameters:\n",
    "            A,B, Variable: Current input to perceptron\n",
    "            y, Variable: Target value for the input\n",
    "        \"\"\"\n",
    "        # forward pass\n",
    "        loss = self.loss_fn(A,B,y)\n",
    "        # backward pass\n",
    "        grads = loss.gradients\n",
    "        # update weights and omit bias\n",
    "        w1 = self.w1 - self.lr*grads[self.w1]\n",
    "        w2 = self.w2 - self.lr*grads[self.w2]\n",
    "        # we need to reinstantiate the Variable object to avoid infinite recursion\n",
    "        self.w1 = Variable(w1.value)\n",
    "        self.w2 = Variable(w2.value)\n",
    "        return loss.value # to track loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8a5944a",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- end solution of exercise --- -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a534bb3b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 2: Training\n",
    "\n",
    "We now train the perceptron for a given number of epochs. We do this by looping over the input-output pairs.\n",
    "For each pair, we compute the loss and perform a gradient step. We then track the loss over the epochs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10c61a86",
   "metadata": {
    "editable": true
   },
   "source": [
    "Fill in the missing code below to finish the train method for the perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f7594",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def train(model, A_B, XOR, epochs=1000):\n",
    "    \"\"\"\n",
    "    Trains the model for a given number of epochs\n",
    "\n",
    "    Parameters:\n",
    "        model, Perceptron2D: The model to train\n",
    "        A_B, np.array: The input to the model\n",
    "        XOR, np.array: The target output of the model\n",
    "        epochs, int: The number of epochs to train for\n",
    "    \"\"\"\n",
    "    loss_history = []\n",
    "    for epoch in tqdm.trange(epochs):\n",
    "        loss = 0\n",
    "        for i in range(len(A_B)):\n",
    "            A, B = A_B[i]\n",
    "            y = XOR[i]\n",
    "            # do a gradient step and track the loss\n",
    "            loss += ___\n",
    "        loss_history.append(loss)\n",
    "    return loss_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec22f68f_1",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35311010",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def train(model, A_B, XOR, epochs=1000):\n",
    "    \"\"\"\n",
    "    Trains the model for a given number of epochs\n",
    "\n",
    "    Parameters:\n",
    "        model, Perceptron2D: The model to train\n",
    "        A_B, np.array: The input to the model\n",
    "        XOR, np.array: The target output of the model\n",
    "        epochs, int: The number of epochs to train for\n",
    "    \"\"\"\n",
    "    loss_history = []\n",
    "    for epoch in tqdm.trange(epochs):\n",
    "        loss = 0\n",
    "        for i in range(len(A_B)):\n",
    "            A, B = A_B[i]\n",
    "            y = XOR[i]\n",
    "            # do a gradient step and track the loss\n",
    "            loss += model.gradient_step(Variable(A), Variable(B), Variable(y))\n",
    "        loss_history.append(loss)\n",
    "    return loss_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8a5944a_1",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- end solution of exercise --- -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c22c9e",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = Perceptron2D()\n",
    "A_B, XOR = xor()\n",
    "loss_history = train(model, A_B, XOR, epochs=1000)\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da2fec64",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Insights into the Perceptron with decision boundaries\n",
    "Now that we have trained the perceptron, we can plot the decision boundary of the perceptron. This is done by\n",
    "evaluating the perceptron on a 2D mesh of the inputs. The decision boundary will be the contour where the perceptron\n",
    "makes the decision to output 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dde2167",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, A_B, XOR, fig=None, ax=None):\n",
    "    \"\"\"\n",
    "    Plots the decision boundary of the model\n",
    "\n",
    "    Parameters:\n",
    "        model, Perceptron2D: The model to plot the decision boundary of\n",
    "        A_B, np.array: The input to the model\n",
    "        XOR, np.array: The target output of the model\n",
    "    \"\"\"\n",
    "    # plot the decision boundary\n",
    "    x = np.linspace(-0.2,1.2,100)\n",
    "    y = np.linspace(-0.2,1.2,100)\n",
    "    X, Y = np.meshgrid(x,y)\n",
    "    Z = np.zeros_like(X)\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X[0])):\n",
    "            Z[i,j] = model.evaluate(Variable(X[i,j]), Variable(Y[i,j]))\n",
    "    if fig is None or ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.contourf(X,Y,Z, alpha=0.2)\n",
    "    ax.scatter(A_B[:,0], A_B[:,1], c=XOR, cmap=\"coolwarm\")\n",
    "    ax.set_xlabel(\"A\")\n",
    "    ax.set_ylabel(\"B\")\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4d5f09",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plot_decision_boundary(model, A_B, XOR)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09bd9ed8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 3: Multi-Layer Perceptron: Understanding Layers and Non-Linearity\n",
    "\n",
    "Our primary focus in this section is to understand two essential questions regarding Multi-Layer Perceptrons (MLP): \n",
    "\"Why do we need multiple layers?\" and \"Why is non-linearity necessary?\".\n",
    "\n",
    "As we have shown, the shortcomings of a single-layer perceptron are evident when dealing with the infamous XOR problem. \n",
    "However, as we will see in this section, we can overcome this limitation by transitioning from a single-layer perceptron to a multi-layer perceptron.\n",
    "\n",
    "Going from a single-layer perceptron to a multi-layer perceptron requires us to address two crucial factors:\n",
    "\n",
    "1. Merely adding another linear layer without introducing non-linearity does NOT increase the expressive power of our model.\n",
    "2. The non-linearity that we introduce must be differentiable and possess a broad [support](https://en.wikipedia.org/wiki/Support_(mathematics)). \n",
    "\n",
    "The second point allows us to use gradient-based optimization techniques, such as variants of gradient descent. \n",
    "Moreover, if the function's gradient is zero in \"large enough\" regions, learning will stop. \n",
    "This consideration highlights that we can't merely expand our previous perceptron using thresholding functions, \n",
    "as its derivative is zero everywhere."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0a0a7ea",
   "metadata": {
    "editable": true
   },
   "source": [
    "**a)**\n",
    "Demonstrate that simply adding another linear layer without incorporating non-linearity does not enhance the expressiveness of the model. \n",
    "To clarify, given a neural network, which can be represented as a composition of functions $f(x) = f_1 \\circ f_2 \\circ ... \\circ f_n (x)$, \n",
    "where each function is linear, i.e., $f_i(z) = W_i z + b_i$, you are required to show that $f(x)$ can be reduced to a single linear transform $W'x + b'$. \n",
    "\n",
    "Hint: Begin by writing the function $f(x)$ explicitly in terms of its weights, biases, and input. \n",
    "Subsequently, simplify and argue that a transformation of this form is equivalent to $W'x + b'$.\n",
    "\n",
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc3b635c",
   "metadata": {
    "editable": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "source": [
    "We can find the solution by following the hint. Expanding the function, we get\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x) &= W_n(W_{n-1}(...(W_1 x + b_1)) + b_{n-1}) + b_n \\\\\n",
    "     &= W_n W_{n-1} ... W_1 x + b_n + W_n b_{n-1} + W_n W_{n-1} b_{n-2} + ... + W_n W_{n-1} ... W_2 b_1 + ... + W_1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here, we can simply choose $W' = W_n W_{n-1} ... W_1$, and $b' = b_n + W_n b_{n-1} + W_n W_{n-1} b_{n-2} + ... + W_n W_{n-1} ... W_2 b_1 + ... + W_1$.\n",
    "<!-- --- end solution of exercise --- -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b1e4b94",
   "metadata": {
    "editable": true
   },
   "source": [
    "**b)**\n",
    "(`OPTIONAL`) In this exercise we want to extend the above argument to neural networks with polynomial activation functions. \n",
    "To get you started, we will first consider a single-layer perceptron with a polynomial activation function of degree 2.\n",
    "1. Consider a two-layer neural network where each layer is a polynomial function of degree 2, in the form $f_i(z) = W_i z^2$ (omitting the bias). \n",
    "    a. Show that this two-layer network can be simplified to a single-layer network with a polynomial function of degree 4. \n",
    "    b. Based on this, argue why the expressiveness of the multi-layer network does not exceed that of the single-layer network.\n",
    "2. In general, argue that a multi-layer perceptron with polynomial activation functions does not have an increased expressiveness over a single-layer perceptron with a polynomial activation function of any degree.\n",
    "`Hint`: Recall that expressiveness refers to the set of functions that a network can represent. \n",
    "A model is said to be more expressive if it can represent a strictly larger set of functions.\n",
    "\n",
    "**Solution.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75ab64ae",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "source": [
    "`Part 1`:\n",
    "Ignoring the bias terms, our two layer neural network function would look like this:\n",
    "\n",
    "$f(x) = f_2(f_1(x)) = W_2(W_1 x^2)^2$\n",
    "\n",
    "Expanding the inner function, we get:\n",
    "\n",
    "$f(x) = W_2(W_1^2 x^4)$\n",
    "\n",
    "So the output of the two-layer network is a scaled fourth degree polynomial.\n",
    "Now, let's compare this to a single-layer network with a fourth degree polynomial activation function:\n",
    "\n",
    "$f(x) = W_4 x^4$\n",
    "\n",
    "If we set $W_4 = W_2W_1^2$, then the single-layer network can represent the same function as the two-layer network. \n",
    "Hence, we have shown that a single-layer network with a fourth degree polynomial activation function can represent the \n",
    "same function as a two-layer network with second degree polynomial functions.\n",
    "\n",
    "`Part 2`:\n",
    "In the context of neural networks, the term \"expressiveness\" refers to the family of functions that a network can approximate. \n",
    "A network is considered more expressive if it can approximate a broader range of functions.\n",
    "\n",
    "In the case of a multi-layer perceptron with polynomial activation functions, it might seem that stacking layers to generate higher degree \n",
    "polynomials would increase the network's expressiveness. However, this is not the case when comparing it to a single-layer network with a \n",
    "sufficiently flexible polynomial activation function.\n",
    "\n",
    "Let's illustrate this using mathematical notation. Let $P_n$ denote the set of all real polynomial functions of degree up to $n$. \n",
    "If we have a single-layer network with an activation function from $P_m$, we can write its function as:\n",
    "\n",
    "$f(x) = W_m x^m + W_{m-1} x^{m-1} + ... + W_1 x + b \\quad f \\in P_m$\n",
    "\n",
    "This function belongs to $P_m$, meaning it can represent any real polynomial function of degree up to $m$.\n",
    "\n",
    "Now, suppose we have a multi-layer perceptron, where each layer is a polynomial function from $P_n$. \n",
    "When stacked together, these layers form a composite function that is in $P_{n^k}$, where $k$ is the number of layers. \n",
    "However, if $m \\geq n^k$, then the single-layer network with activation function from $P_m$ can represent the same set of functions as \n",
    "the multi-layer network.\n",
    "\n",
    "Therefore, adding more layers to a multi-layer perceptron doesn't inherently increase the expressiveness beyond what a single-layer network \n",
    "with a polynomial activation function from $P_m$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "833acfdf",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The sigmoid function\n",
    "As a result, we must identify a different non-linear function to replace our thresholding function. \n",
    "Traditionally, the sigmoid function, defined as"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "377fc5d5",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\sigma(x) = \\frac{1}{1+\\exp{-x}}, \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e49b1f6b",
   "metadata": {
    "editable": true
   },
   "source": [
    "is selected for this purpose. \n",
    "This function bears a similarity to the thresholding function and can be thought of as a smoother (differentiable) version of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ba77b1",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.linspace(-5,5,100)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"$\\sigma(x)$\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84fde267",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The XOR problem reformulated\n",
    "While it is normally not clear how many layers and neurons are needed to approximate a given function, \n",
    "for the XOR problem we can show that a two-layer perceptron with two neurons in each layer is sufficient.\n",
    "While this is interesting, we omit the details of (which can be found in the resource section), and instead\n",
    "just state the result: XOR can be written as,"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61c98700",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "XOR(A,B) = (A + B) \\cdot \\bar{AB}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba1c8257",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $+$ represents the logical OR operation, and $\\bar{AB}$ denotes the logical NAND operation. \n",
    "It is important to note that both NAND and OR are linearly separable functions, \n",
    "which means they can be approximated by individual single-layer 2D-perceptrons. \n",
    "Furthermore, since the output of each function is a single bit, \n",
    "they can be combined using the logical AND operation, which is also linearly separable. \n",
    "By combining these components, we can create a two-layer perceptron capable of approximating the XOR function.\n",
    "\n",
    "Confirm by yourself that you can linearly separate the XOR function using the logical OR, NAND, and AND operations\n",
    "by looking at the following plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d29f9b",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "A, B = A_B.T\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "ax[0].scatter(A, B, c=A|B, cmap='bwr')\n",
    "ax[0].set_title(\"OR\")\n",
    "ax[1].scatter(A, B, c=~(A&B), cmap='bwr')\n",
    "ax[1].set_title(\"NAND\")\n",
    "ax[2].scatter(A, B, c=A&B, cmap='bwr')\n",
    "ax[2].set_title(\"AND\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "651c50c3",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 4: Implementing a two-layer perceptron\n",
    "\n",
    "Let us now implement a two-layer perceptron in Python, again using the Variable class from the previous section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a52a1a7",
   "metadata": {
    "editable": true
   },
   "source": [
    "Fill in the missing code in the `forward` function to finish the implementation of a two-layer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f014234a",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, T=0.5, lr=1e-1):\n",
    "        \"\"\"\n",
    "        A two layer perceptron with two neurons in each layer\n",
    "        \"\"\"\n",
    "        self.T, self.lr = T, lr\n",
    "        # initialize weights and bias for first layer\n",
    "        # hidden neuron one\n",
    "        self.w11 = Variable(np.random.normal(scale=0.1))\n",
    "        self.w12 = Variable(np.random.normal(scale=0.1))\n",
    "        self.b11 = Variable(np.random.normal(scale=0.01))\n",
    "        # hidden neuron two\n",
    "        self.w13 = Variable(np.random.normal(scale=0.1))\n",
    "        self.w14 = Variable(np.random.normal(scale=0.1))\n",
    "        self.b12 = Variable(np.random.normal(scale=0.01))\n",
    "        # initialize weights and bias for second layer\n",
    "        # output neuron\n",
    "        self.w21 = Variable(np.random.normal(scale=0.1))\n",
    "        self.w22 = Variable(np.random.normal(scale=0.1))\n",
    "        self.b21 = Variable(np.random.normal(scale=0.01))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Recreate the sigmoid function using the Variable class\n",
    "        \"\"\"\n",
    "        from variable import exp\n",
    "        return 1 / (1 + exp(-x))\n",
    "    \n",
    "    def forward(self,A,B):\n",
    "        \"\"\"\n",
    "        Forward of the MLP for a given input\n",
    "\n",
    "        Parameters:\n",
    "            A,B, Variable: Current input to the MLP\n",
    "        Returns:\n",
    "            y_hat, Variable: A Variable object following the perceptron auto-diff graph\n",
    "        \"\"\"\n",
    "        #\n",
    "        # ----> FILL IN MISSING CODE HERE <----\n",
    "        #\n",
    "        return NotImplementedError\n",
    "\n",
    "    def evaluate(self,A,B):\n",
    "        \"\"\"\n",
    "        Evaluates the MLP for a given input - thresholds the forward pass\n",
    "\n",
    "        Parameters:\n",
    "            A,B, Variable: Current input to perceptron\n",
    "        Returns:\n",
    "            y_hat, Variable: A Variable object following the perceptron auto-diff graph\n",
    "        \"\"\"\n",
    "        y_hat = self.forward(A,B)\n",
    "        return y_hat.value > self.T\n",
    "\n",
    "    def loss_fn(self,A,B,y):\n",
    "        \"\"\"\n",
    "        Computes the loss function for a given input\n",
    "\n",
    "        Parameters:\n",
    "            A,B, Variable: Current input to MLP\n",
    "            y, Variable: Target value for the input\n",
    "        Returns:\n",
    "            loss, Variable: A Variable object following the loss auto-diff graph\n",
    "        \"\"\"\n",
    "        y_hat = self.forward(A,B)\n",
    "        loss = (y_hat - y)**2\n",
    "        return loss\n",
    "\n",
    "    def gradient_step(self, A, B, y):\n",
    "        \"\"\"\n",
    "        Performs a gradient step for a given input\n",
    "\n",
    "        Parameters:\n",
    "            A,B, Variable: Current input to MLP\n",
    "            y, Variable: Target value for the input\n",
    "        \"\"\"\n",
    "        # forward pass\n",
    "        loss = self.loss_fn(A,B,y)\n",
    "        # backward pass\n",
    "        grads = loss.gradients\n",
    "        # update weights and bias\n",
    "        self.w11 = Variable((self.w11 - self.lr*grads[self.w11]).value)\n",
    "        self.w12 = Variable((self.w12 - self.lr*grads[self.w12]).value)\n",
    "        self.w13 = Variable((self.w13 - self.lr*grads[self.w13]).value)\n",
    "        self.w14 = Variable((self.w14 - self.lr*grads[self.w14]).value)\n",
    "        self.b11 = Variable((self.b11 - self.lr*grads[self.b11]).value)\n",
    "        self.b12 = Variable((self.b12 - self.lr*grads[self.b12]).value)\n",
    "        self.w21 = Variable((self.w21 - self.lr*grads[self.w21]).value)\n",
    "        self.w22 = Variable((self.w22 - self.lr*grads[self.w22]).value)\n",
    "        self.b21 = Variable((self.b21 - self.lr*grads[self.b21]).value)\n",
    "        return loss.value # to track loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec22f68f_2",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33927c22",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, T=0.5, lr=1e-1):\n",
    "        \"\"\"\n",
    "        A two layer perceptron with two neurons in each layer\n",
    "        \"\"\"\n",
    "        self.T, self.lr = T, lr\n",
    "        # initialize weights and bias for first layer\n",
    "        # hidden neuron one\n",
    "        self.w11 = Variable(np.random.normal(scale=0.1))\n",
    "        self.w12 = Variable(np.random.normal(scale=0.1))\n",
    "        self.b11 = Variable(np.random.normal(scale=0.01))\n",
    "        # hidden neuron two\n",
    "        self.w13 = Variable(np.random.normal(scale=0.1))\n",
    "        self.w14 = Variable(np.random.normal(scale=0.1))\n",
    "        self.b12 = Variable(np.random.normal(scale=0.01))\n",
    "        # initialize weights and bias for second layer\n",
    "        # output neuron\n",
    "        self.w21 = Variable(np.random.normal(scale=0.1))\n",
    "        self.w22 = Variable(np.random.normal(scale=0.1))\n",
    "        self.b21 = Variable(np.random.normal(scale=0.01))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Recreate the sigmoid function using the Variable class\n",
    "        \"\"\"\n",
    "        from variable import exp\n",
    "        return 1 / (1 + exp(-x))\n",
    "    \n",
    "    def forward(self,A,B):\n",
    "        \"\"\"\n",
    "        Forward of the MLP for a given input\n",
    "\n",
    "        Parameters:\n",
    "            A,B, Variable: Current input to the MLP\n",
    "        Returns:\n",
    "            y_hat, Variable: A Variable object following the perceptron auto-diff graph\n",
    "        \"\"\"\n",
    "        # first layer\n",
    "        z1 = A*self.w11 + B*self.w12 + self.b11 # \"first perceptron\"\n",
    "        z2 = A*self.w13 + B*self.w14 + self.b12 # \"second perceptron\"\n",
    "        y1 = self.sigmoid(z1)\n",
    "        y2 = self.sigmoid(z2)\n",
    "        # second layer\n",
    "        y_hat = y1*self.w21 + y2*self.w22 + self.b21 # \"third perceptron\"\n",
    "        return self.sigmoid(y_hat)\n",
    "\n",
    "    def evaluate(self,A,B):\n",
    "        \"\"\"\n",
    "        Evaluates the MLP for a given input - thresholds the forward pass\n",
    "\n",
    "        Parameters:\n",
    "            A,B, Variable: Current input to perceptron\n",
    "        Returns:\n",
    "            y_hat, Variable: A Variable object following the perceptron auto-diff graph\n",
    "        \"\"\"\n",
    "        y_hat = self.forward(A,B)\n",
    "        return y_hat.value > self.T\n",
    "\n",
    "    def loss_fn(self,A,B,y):\n",
    "        \"\"\"\n",
    "        Computes the loss function for a given input\n",
    "\n",
    "        Parameters:\n",
    "            A,B, Variable: Current input to MLP\n",
    "            y, Variable: Target value for the input\n",
    "        Returns:\n",
    "            loss, Variable: A Variable object following the loss auto-diff graph\n",
    "        \"\"\"\n",
    "        y_hat = self.forward(A,B)\n",
    "        loss = (y_hat - y)**2\n",
    "        return loss\n",
    "\n",
    "    def gradient_step(self, A, B, y):\n",
    "        \"\"\"\n",
    "        Performs a gradient step for a given input\n",
    "\n",
    "        Parameters:\n",
    "            A,B, Variable: Current input to MLP\n",
    "            y, Variable: Target value for the input\n",
    "        \"\"\"\n",
    "        # forward pass\n",
    "        loss = self.loss_fn(A,B,y)\n",
    "        # backward pass\n",
    "        grads = loss.gradients\n",
    "        # update weights and bias\n",
    "        self.w11 = Variable((self.w11 - self.lr*grads[self.w11]).value)\n",
    "        self.w12 = Variable((self.w12 - self.lr*grads[self.w12]).value)\n",
    "        self.w13 = Variable((self.w13 - self.lr*grads[self.w13]).value)\n",
    "        self.w14 = Variable((self.w14 - self.lr*grads[self.w14]).value)\n",
    "        self.b11 = Variable((self.b11 - self.lr*grads[self.b11]).value)\n",
    "        self.b12 = Variable((self.b12 - self.lr*grads[self.b12]).value)\n",
    "        self.w21 = Variable((self.w21 - self.lr*grads[self.w21]).value)\n",
    "        self.w22 = Variable((self.w22 - self.lr*grads[self.w22]).value)\n",
    "        self.b21 = Variable((self.b21 - self.lr*grads[self.b21]).value)\n",
    "        return loss.value # to track loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8a5944a_2",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- end solution of exercise --- -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09eeb400",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Decision Boundary\n",
    "Let us now visualize the decision boundary of the MLP to see how it performs on the XOR problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebb628b",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2)\n",
    "\n",
    "# train model\n",
    "model = MLP(lr=1e-1)\n",
    "loss_history = train(model, A_B, XOR, epochs=10000)\n",
    "\n",
    "# plot loss and decision boundary\n",
    "axs[0].plot(loss_history)\n",
    "plot_decision_boundary(model, A_B, XOR, fig=fig, ax=axs[1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ca2mec",
   "language": "python",
   "name": "ca2mec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
