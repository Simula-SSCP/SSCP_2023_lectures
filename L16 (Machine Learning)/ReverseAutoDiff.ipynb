{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "381bdbde",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- File automatically generated using DocOnce (https://github.com/doconce/doconce/):\n",
    "doconce format ipynb ReverseAutoDiff.do.txt  -->\n",
    "\n",
    "## Reverse-mode automatic differentiation\n",
    "Deep learning frameworks are built upon the foundation of automatic differentiation. \n",
    "Training deep learning models typically involves gradient-based techniques, \n",
    "with autodiff streamlining the gradient acquisition process, even for large and intricate models.\n",
    "The majority of deep learning frameworks utilize 'reverse-mode autodiff' due to its efficiency and precision.\n",
    "\n",
    "In this module, we will delve into the generalization of the chain rule for automatic differentiation of any function. \n",
    "This is achieved by understanding that all functions are composed of basic operations such as addition, multiplication, \n",
    "subtraction, and division.\n",
    "\n",
    "**Notice.**\n",
    "\n",
    "The initial examples and code base are modified from <https://sidsite.com/posts/autodiff/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d135d69",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (4, 2)\n",
    "plt.rcParams['figure.dpi'] = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cd623f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Simple example\n",
    "We begin with a simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9519da2a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "a &= 4\\\\\n",
    "b &= 3\\\\\n",
    "c &= a + b\\\\\n",
    "d &= a c\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462c226c",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- dom:FIGURE: [figures/auto-diff-simple-graph.png] <div id=\"fig:auto-diff-simple-graph\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:auto-diff-simple-graph\"></div>\n",
    "\n",
    "<img src=\"figures/auto-diff-simple-graph.png\" ><p style=\"font-size: 0.9em\"><i>Figure 1</i></p>\n",
    "<!-- end figure -->\n",
    "\n",
    "First work through a simple example and implement a minimal version of a variable class and a method to compute gradients.\n",
    "Then we will move over to the linear regression example and proceed to build a full \n",
    "MLP model to solve a more complicated example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7564a5d7",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 1: Compute the partial derivatives\n",
    "\n",
    "To do so we have created a `Variable` class that stores the value of the variable and the gradients with respect to its children.\n",
    "The gradients are stored as a tuple of tuples, where each tuple contains a reference to the child variable and the local \n",
    "derivative with respect to that child variable.\n",
    "\n",
    "Fill in the missing code in the `mul` function to compute the gradients of the variables with respect to their children.\n",
    "Then fill in the missing code in the `compute_gradients` function to compute the gradients of the variables with respect to their children."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e11d1d",
   "metadata": {
    "editable": true
   },
   "source": [
    "**a)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fb3f6e",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class Variable:\n",
    "    def __init__(self, value, gradients=None):\n",
    "        self.value = value\n",
    "        self._gradients = gradients if gradients is not None else ((self, np.sign(value)),)\n",
    "        self._stored_gradients = None\n",
    "\n",
    "    @property\n",
    "    def gradients(self):\n",
    "        return compute_gradients(self)\n",
    "    \n",
    "def add(a, b):\n",
    "    \"Create the variable that results from adding two variables.\"\n",
    "    value = a.value + b.value    \n",
    "    gradients = (\n",
    "        (a, 1),  # the local derivative with respect to a is 1\n",
    "        (b, 1)   # the local derivative with respect to b is 1\n",
    "    )\n",
    "    return Variable(value, gradients)\n",
    "\n",
    "def mul(a, b):\n",
    "    \"Create the variable that results from multiplying two variables.\"\n",
    "    # ---> TODO: fill in the missing code <---\n",
    "    value = ___\n",
    "    gradients = (\n",
    "        (a, ___), # the local derivative with respect to a is b.value\n",
    "        (b, ___)  # the local derivative with respect to b is a.value\n",
    "    )\n",
    "    return Variable(value, gradients)\n",
    "\n",
    "def compute_gradients(variable):\n",
    "    \"\"\" Compute the first derivatives of `variable` \n",
    "    with respect to child variables.\n",
    "    \"\"\"\n",
    "    gradients = defaultdict(lambda: 0)\n",
    "    \n",
    "    def _compute_gradients(variable, total_gradient):\n",
    "        for child_variable, child_gradient in variable._gradients:\n",
    "            # ---> TODO: fill in the missing code <---\n",
    "            # \"Multiply the edges of a path\":\n",
    "            gradient = ___\n",
    "            # \"Add together the different paths\":\n",
    "            gradients[child_variable] = ___\n",
    "\n",
    "            # if the child variable only has itself as a gradient \n",
    "            # we have reached the end of recursion\n",
    "            criteria = (\n",
    "                len(child_variable._gradients) == 1 and \n",
    "                child_variable._gradients[0][0] is child_variable\n",
    "            )\n",
    "            if not criteria:\n",
    "                # recurse through graph:\n",
    "                _compute_gradients(child_variable, gradient)\n",
    "    \n",
    "    _compute_gradients(variable, total_gradient=1)\n",
    "    # (total_gradient=1 is from `variable` differentiated w.r.t. itself)\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22f68f",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f3d893",
   "metadata": {
    "editable": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class Variable:\n",
    "    def __init__(self, value, gradients=None):\n",
    "        self.value = value\n",
    "        self._gradients = gradients if gradients is not None else ((self, np.sign(value)),)\n",
    "        self._stored_gradients = None\n",
    "\n",
    "    @property\n",
    "    def gradients(self):\n",
    "        if self._stored_gradients is None:\n",
    "            self._stored_gradients = dict(compute_gradients(self))\n",
    "        return self._stored_gradients\n",
    "    \n",
    "def add(a, b):\n",
    "    \"Create the variable that results from adding two variables.\"\n",
    "    value = a.value + b.value    \n",
    "    gradients = (\n",
    "        (a, 1),  # the local derivative with respect to a is 1\n",
    "        (b, 1)   # the local derivative with respect to b is 1\n",
    "    )\n",
    "    return Variable(value, gradients)\n",
    "\n",
    "def mul(a, b):\n",
    "    \"Create the variable that results from multiplying two variables.\"\n",
    "    value = a.value * b.value\n",
    "    gradients = (\n",
    "        (a, b.value), # the local derivative with respect to a is b.value\n",
    "        (b, a.value)  # the local derivative with respect to b is a.value\n",
    "    )\n",
    "    return Variable(value, gradients)\n",
    "\n",
    "def compute_gradients(variable):\n",
    "    \"\"\" Compute the first derivatives of `variable` \n",
    "    with respect to child variables.\n",
    "    \"\"\"\n",
    "    gradients = defaultdict(lambda: 0)\n",
    "    \n",
    "    def _compute_gradients(variable, total_gradient):\n",
    "        for child_variable, child_gradient in variable._gradients:\n",
    "            # \"Multiply the edges of a path\":\n",
    "            gradient = total_gradient * child_gradient\n",
    "            # \"Add together the different paths\":\n",
    "            gradients[child_variable] += gradient\n",
    "            # if the child variable only has itself as a gradient \n",
    "            # we have reached the end of recursion\n",
    "            criteria = (\n",
    "                len(child_variable._gradients) == 1 and \n",
    "                child_variable._gradients[0][0] is child_variable\n",
    "            )\n",
    "            if not criteria:\n",
    "                # recurse through graph:\n",
    "                _compute_gradients(child_variable, gradient)\n",
    "    \n",
    "    _compute_gradients(variable, total_gradient=1)\n",
    "    # (total_gradient=1 is from `variable` differentiated w.r.t. itself)\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a5944a",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- end solution of exercise --- -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68c5695",
   "metadata": {
    "editable": true
   },
   "source": [
    "**b)**\n",
    "Test function against true value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a74e83f",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "a = Variable(4)\n",
    "# TODO: fill in the missing code\n",
    "b = ___\n",
    "c = ___\n",
    "d = ___\n",
    "\n",
    "assert d.gradients[a] == ___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22f68f_1",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac99058",
   "metadata": {
    "editable": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "a = Variable(4)\n",
    "b = Variable(3)\n",
    "c = add(a, b)\n",
    "d = mul(a, c)\n",
    "\n",
    "assert d.gradients[a] == 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a5944a_1",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- end solution of exercise --- -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f52b27",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Linear rate neurons\n",
    "\n",
    "Starting with the loss $\\mathcal{L}=\\frac{1}{2}(y - \\hat{y})^2$ we have the \n",
    "\n",
    "<!-- dom:FIGURE: [figures/auto-diff-graph.png] <div id=\"fig:auto-diff-graph\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:auto-diff-graph\"></div>\n",
    "\n",
    "<img src=\"figures/auto-diff-graph.png\" ><p style=\"font-size: 0.9em\"><i>Figure 2</i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f18531",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 2: Validate the gradients\n",
    "\n",
    "Run the following code to validate the manually computed gradients to the ones computed by automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40f9f9b",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss(y, y_hat):\n",
    "    return 0.5 * (y - y_hat)**2\n",
    "\n",
    "def sigma(x):\n",
    "    return 1. / (1. + exp(-x))\n",
    "\n",
    "def y_hat(w_0, w_1, x_1):\n",
    "    return sigma(w_0 + w_1*x_1)\n",
    "\n",
    "def dy_hat_dw_1(w_0, w_1, x_1):\n",
    "    return x_1*y_hat(w_0, w_1, x_1) * (1 - y_hat(w_0, w_1, x_1))\n",
    "\n",
    "def dloss_dw_1(y, w_0, w_1, x_1):\n",
    "    return (y_hat(w_0, w_1, x_1) - y) * dy_hat_dw_1(w_0, w_1, x_1)\n",
    "\n",
    "def linear_y_hat(w_0, w_1, x_1):\n",
    "    return w_0 + w_1*x_1\n",
    "\n",
    "def linear_dL_dw0(y, y_hat):\n",
    "    return y_hat - y\n",
    "\n",
    "def linear_dL_dw1(y, y_hat, x_1):\n",
    "    return x_1*(y_hat - y)\n",
    "\n",
    "x_1 = Variable(0.1, name='x_1')\n",
    "w_0 = Variable(4, name='w_0')\n",
    "w_1 = Variable(3, name='w_1')\n",
    "y = Variable(10, name='y')\n",
    "\n",
    "\n",
    "assert isclose(compute_gradients(\n",
    "    loss(y, y_hat(w_0, w_1, x_1)))[y], y - y_hat(w_0, w_1, x_1)\n",
    "    )\n",
    "assert isclose(\n",
    "    compute_gradients(y_hat(w_0, w_1, x_1))[w_1], dy_hat_dw_1(w_0, w_1, x_1)\n",
    "    )\n",
    "assert isclose(\n",
    "    compute_gradients(loss(y, y_hat(w_0, w_1, x_1)))[w_1], dloss_dw_1(y, w_0, w_1, x_1)\n",
    "    )\n",
    "assert isclose(\n",
    "    compute_gradients(loss(y, linear_y_hat(w_0, w_1, x_1)))[w_1], linear_dL_dw1(y, linear_y_hat(w_0, w_1, x_1), x_1)\n",
    "    )\n",
    "assert isclose(\n",
    "    compute_gradients(loss(y, linear_y_hat(w_0, w_1, x_1)))[w_0], linear_dL_dw0(y, linear_y_hat(w_0, w_1, x_1))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201561c4-c40b-4585-8676-21a0e119ff28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
