{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "381bdbde",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- File automatically generated using DocOnce (https://github.com/doconce/doconce/):\n",
    "doconce format ipynb ReverseAutoDiff.do.txt  -->\n",
    "\n",
    "## Reverse-mode automatic differentiation\n",
    "Deep learning frameworks are built upon the foundation of automatic differentiation. \n",
    "Training deep learning models typically involves gradient-based techniques, \n",
    "with autodiff streamlining the gradient acquisition process, even for large and intricate models.\n",
    "The majority of deep learning frameworks utilize 'reverse-mode autodiff' due to its efficiency and precision.\n",
    "\n",
    "In this module, we will delve into the generalization of the chain rule for automatic differentiation of any function. \n",
    "This is achieved by understanding that all functions are composed of basic operations such as addition, multiplication, \n",
    "subtraction, and division.\n",
    "\n",
    "**Notice.**\n",
    "\n",
    "The initial examples and code base are modified from <https://sidsite.com/posts/autodiff/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb7ceb6b",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (4, 2)\n",
    "plt.rcParams['figure.dpi'] = 150"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dff17c9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 1: Simple example\n",
    "\n",
    "We begin with a simple example where we want to compute the gradients of a function $d$ with respect to its children $a$ and $b$.\n",
    "The point of this exercise is to show that you can use a computational graph directly to state the gradients.\n",
    "Given the function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9519da2a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "a &= 4\\\\\n",
    "b &= 3\\\\\n",
    "c &= a + b\\\\\n",
    "d &= a c\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92d2cb4a",
   "metadata": {
    "editable": true
   },
   "source": [
    "we can build a computational graph of the function $d$ shown in the figure below:\n",
    "\n",
    "<!-- dom:FIGURE: [figures/auto-diff-simple-graph.png] <div id=\"fig:auto-diff-simple-graph\"></div> The function $d$ is composed of basic operations (addition, multiplication). By representing the function as a graph, we can visualize the flow of information through the function. When computing gradients, we start at the end of the graph and work our way backwards, multiplying the gradients of the children and adding the branches.  -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:auto-diff-simple-graph\"></div>\n",
    "\n",
    "<img src=\"figures/auto-diff-simple-graph.png\" ><p style=\"font-size: 0.9em\"><i>Figure 1:  The function $d$ is composed of basic operations (addition, multiplication). By representing the function as a graph, we can visualize the flow of information through the function. When computing gradients, we start at the end of the graph and work our way backwards, multiplying the gradients of the children and adding the branches.</i></p>\n",
    "<!-- end figure -->\n",
    "\n",
    "Work through this simple example and find the gradients $\\frac{\\partial d}{\\partial a}$ and $\\frac{\\partial d}{\\partial b}$ using the graph.\n",
    "\n",
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa6684f8",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-ouput",
     "hide-input"
    ]
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial d}{\\partial a} = c + a\\frac{\\partial c}{\\partial a}\n",
    "\\label{_auto1} \\tag{1}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d64670d1",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-ouput",
     "hide-input"
    ]
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "\\frac{\\partial d}{\\partial b} = \\frac{\\partial d}{\\partial c}\\frac{\\partial c}{\\partial b}\n",
    "\\label{_auto2} \\tag{2}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8a5944a",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- end solution of exercise --- -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c8e43e6",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 2: Compute the partial derivatives automatically\n",
    "\n",
    "Work through this simple example and implement a minimal version of a variable class and a method to compute gradients.\n",
    "\n",
    "To do so we have created a `Variable` class that stores the value of the variable and the gradients with respect to its children.\n",
    "The gradients are stored as a tuple of tuples, where each tuple contains a reference to the child variable and it correspoding partial\n",
    "derivative.\n",
    "\n",
    "Fill in the missing code in the `mul` function to compute the gradients of the variables with respect to their children.\n",
    "Then fill in the missing code in the `compute_gradients` function to compute the gradients of the variables with respect to their children."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26e11d1d",
   "metadata": {
    "editable": true
   },
   "source": [
    "**a)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5fb3f6e",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class Variable:\n",
    "    def __init__(self, value, gradients=None):\n",
    "        self.value = value\n",
    "        self._gradients = gradients if gradients is not None else ((self, np.sign(value)),)\n",
    "        self._stored_gradients = None\n",
    "\n",
    "    @property\n",
    "    def gradients(self):\n",
    "        return compute_gradients(self)\n",
    "    \n",
    "def add(a, b):\n",
    "    \"Create the variable that results from adding two variables.\"\n",
    "    value = a.value + b.value    \n",
    "    gradients = (\n",
    "        (a, 1),  # the local derivative with respect to a is 1\n",
    "        (b, 1)   # the local derivative with respect to b is 1\n",
    "    )\n",
    "    return Variable(value, gradients)\n",
    "\n",
    "def mul(a, b):\n",
    "    \"Create the variable that results from multiplying two variables.\"\n",
    "    # ---> TODO: fill in the missing code <---\n",
    "    value = ___\n",
    "    gradients = (\n",
    "        (a, ___), # the local derivative with respect to a is b.value\n",
    "        (b, ___)  # the local derivative with respect to b is a.value\n",
    "    )\n",
    "    return Variable(value, gradients)\n",
    "\n",
    "def compute_gradients(variable):\n",
    "    \"\"\" Compute the first derivatives of `variable` \n",
    "    with respect to child variables.\n",
    "    \"\"\"\n",
    "    gradients = defaultdict(lambda: 0)\n",
    "    \n",
    "    def _compute_gradients(variable, total_gradient):\n",
    "        for child_variable, child_gradient in variable._gradients:\n",
    "            # ---> TODO: fill in the missing code <---\n",
    "            # \"Multiply the edges of a path\":\n",
    "            gradient = ___\n",
    "            # \"Add together the different paths\":\n",
    "            gradients[child_variable] = ___\n",
    "\n",
    "            # if the child variable only has itself as a gradient \n",
    "            # we have reached the end of recursion\n",
    "            criteria = (\n",
    "                len(child_variable._gradients) == 1 and \n",
    "                child_variable._gradients[0][0] is child_variable\n",
    "            )\n",
    "            if not criteria:\n",
    "                # recurse through graph:\n",
    "                _compute_gradients(child_variable, gradient)\n",
    "    \n",
    "    _compute_gradients(variable, total_gradient=1)\n",
    "    # (total_gradient=1 is from `variable` differentiated w.r.t. itself)\n",
    "    return gradients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec22f68f",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcd6bb97",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-ouput",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class Variable:\n",
    "    def __init__(self, value, gradients=None):\n",
    "        self.value = value\n",
    "        self._gradients = gradients if gradients is not None else ((self, np.sign(value)),)\n",
    "        self._stored_gradients = None\n",
    "\n",
    "    @property\n",
    "    def gradients(self):\n",
    "        if self._stored_gradients is None:\n",
    "            self._stored_gradients = dict(compute_gradients(self))\n",
    "        return self._stored_gradients\n",
    "    \n",
    "def add(a, b):\n",
    "    \"Create the variable that results from adding two variables.\"\n",
    "    value = a.value + b.value    \n",
    "    gradients = (\n",
    "        (a, 1),  # the partial derivative with respect to a is 1\n",
    "        (b, 1)   # the partial derivative with respect to b is 1\n",
    "    )\n",
    "    return Variable(value, gradients)\n",
    "\n",
    "def mul(a, b):\n",
    "    \"Create the variable that results from multiplying two variables.\"\n",
    "    value = a.value * b.value\n",
    "    gradients = (\n",
    "        (a, b.value), # the partial derivative with respect to a is b.value\n",
    "        (b, a.value)  # the partial derivative with respect to b is a.value\n",
    "    )\n",
    "    return Variable(value, gradients)\n",
    "\n",
    "def compute_gradients(variable):\n",
    "    \"\"\" Compute the first derivatives of `variable` \n",
    "    with respect to child variables.\n",
    "    \"\"\"\n",
    "    gradients = defaultdict(lambda: 0)\n",
    "    \n",
    "    def _compute_gradients(variable, total_gradient):\n",
    "        for child_variable, child_gradient in variable._gradients:\n",
    "            # \"Multiply the edges of a path\":\n",
    "            gradient = total_gradient * child_gradient\n",
    "            # \"Add together the different paths\":\n",
    "            gradients[child_variable] += gradient\n",
    "            # if the child variable only has itself as a gradient \n",
    "            # we have reached the end of recursion\n",
    "            criteria = (\n",
    "                len(child_variable._gradients) == 1 and \n",
    "                child_variable._gradients[0][0] is child_variable\n",
    "            )\n",
    "            if not criteria:\n",
    "                # recurse through graph:\n",
    "                _compute_gradients(child_variable, gradient)\n",
    "    \n",
    "    _compute_gradients(variable, total_gradient=1)\n",
    "    # (total_gradient=1 is from `variable` differentiated w.r.t. itself)\n",
    "    return gradients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8a5944a_1",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- end solution of exercise --- -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f68c5695",
   "metadata": {
    "editable": true
   },
   "source": [
    "**b)**\n",
    "Test function against true value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a74e83f",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a = Variable(4)\n",
    "# TODO: fill in the missing code\n",
    "b = ___\n",
    "c = ___\n",
    "d = ___\n",
    "\n",
    "assert d.gradients[a] == ___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec22f68f_1",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ac99058",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-ouput",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "a = Variable(4)\n",
    "b = Variable(3)\n",
    "c = add(a, b)\n",
    "d = mul(a, c)\n",
    "\n",
    "assert d.gradients[a] == 11"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8a5944a_2",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- end solution of exercise --- -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5631591d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Linear rate neurons\n",
    "\n",
    "Starting with the loss $\\mathcal{L}=\\frac{1}{2}(y - \\hat{y})^2$ we have the computation graph in the figure below.\n",
    "\n",
    "<!-- dom:FIGURE: [figures/auto-diff-graph.png] <div id=\"fig:auto-diff-graph\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:auto-diff-graph\"></div>\n",
    "\n",
    "<img src=\"figures/auto-diff-graph.png\" ><p style=\"font-size: 0.9em\"><i>Figure 2</i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de4416f8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 3: Validate the gradients\n",
    "\n",
    "In this exercise we want to show the benefit of autodiff. \n",
    "In the previous module: LinearRegression, we saw that we had to compute gradients of the function manually - this is what autodiff is gonna automate for us. \n",
    "We will simply define the forward pass of our function (**fill in the blanks `___` in the functions `loss`, `sigma` and `y_hat` below**).\n",
    "In this exercise we will use a predefined Variable class from `variable.py` to automatically compute its gradient. \n",
    "We will compare with the ground truth analytical gradient (we restrict ourselves to compare the partial derivative of one variable $w_1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "449fa300",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import a \"complete\" version of the above Variable class.\n",
    "from variable import *\n",
    "\n",
    "def loss(y, y_hat):\n",
    "    return ___ # <---- FILL IN\n",
    "\n",
    "def sigma(x):\n",
    "    return ___ # <---- FILL IN\n",
    "\n",
    "def y_hat(w_0, w_1, x_1):\n",
    "    return ___ # <---- FILL IN\n",
    "\n",
    "def dy_hat_dw_1(w_0, w_1, x_1):\n",
    "    return x_1*y_hat(w_0, w_1, x_1) * (1 - y_hat(w_0, w_1, x_1))\n",
    "\n",
    "def dloss_dw_1(y, w_0, w_1, x_1):\n",
    "    return (y_hat(w_0, w_1, x_1) - y) * dy_hat_dw_1(w_0, w_1, x_1)\n",
    "\n",
    "x_1 = Variable(0.1, name='x_1')\n",
    "w_0 = Variable(4, name='w_0')\n",
    "w_1 = Variable(3, name='w_1')\n",
    "y = Variable(10, name='y')\n",
    "\n",
    "def isclose(a, b):\n",
    "    \"\"\"\n",
    "    Checks if two variables are similiar\n",
    "    \"\"\"\n",
    "    a = a if not isinstance(a, Variable) else a.value\n",
    "    b = b if not isinstance(b, Variable) else b.value\n",
    "    return np.isclose(a, b)\n",
    "\n",
    "# Test if analytic and autodiff for computing d_loss/d_w_1 is similar\n",
    "assert isclose(loss(y, y_hat(w_0, w_1, x_1)).gradients[w_1], dloss_dw_1(y, w_0, w_1, x_1))\n",
    "print('success!!!!!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec22f68f_2",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- begin solution of exercise --- -->\n",
    "**Solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "989d4cd2",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-ouput",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# import a \"complete\" version of the above Variable class.\n",
    "from variable import *\n",
    "\n",
    "def loss(y, y_hat):\n",
    "    return 0.5 * (y - y_hat)**2\n",
    "\n",
    "def sigma(x):\n",
    "    return 1. / (1. + exp(-x))\n",
    "\n",
    "def y_hat(w_0, w_1, x_1):\n",
    "    return sigma(w_0 + w_1*x_1)\n",
    "\n",
    "def dy_hat_dw_1(w_0, w_1, x_1):\n",
    "    return x_1*y_hat(w_0, w_1, x_1) * (1 - y_hat(w_0, w_1, x_1))\n",
    "\n",
    "def dloss_dw_1(y, w_0, w_1, x_1):\n",
    "    return (y_hat(w_0, w_1, x_1) - y) * dy_hat_dw_1(w_0, w_1, x_1)\n",
    "\n",
    "x_1 = Variable(0.1, name='x_1')\n",
    "w_0 = Variable(4, name='w_0')\n",
    "w_1 = Variable(3, name='w_1')\n",
    "y = Variable(10, name='y')\n",
    "\n",
    "def isclose(a, b):\n",
    "    \"\"\"\n",
    "    Checks if two variables are similiar\n",
    "    \"\"\"\n",
    "    a = a if not isinstance(a, Variable) else a.value\n",
    "    b = b if not isinstance(b, Variable) else b.value\n",
    "    return np.isclose(a, b)\n",
    "\n",
    "# Test if analytic and autodiff for computing d_loss/d_w_1 is similar\n",
    "assert isclose(loss(y, y_hat(w_0, w_1, x_1)).gradients[w_1], dloss_dw_1(y, w_0, w_1, x_1))\n",
    "print('success!!!!!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8a5944a_3",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- --- end solution of exercise --- -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doconce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
